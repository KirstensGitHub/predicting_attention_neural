{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "useful-watts",
   "metadata": {},
   "source": [
    "# First level GLM - Veridical versus Scrambled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-senator",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposite-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn as nl\n",
    "import datetime\n",
    "from nilearn.plotting import view_img\n",
    "import numpy.ma as ma\n",
    "\n",
    "from nilearn import image\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "from nilearn.interfaces import fmriprep\n",
    "from nilearn.image import load_img\n",
    "from nilearn.signal import clean\n",
    "from nilearn.image import resample_img\n",
    "from nilearn.signal import clean\n",
    "from nilearn.masking import compute_brain_mask\n",
    "\n",
    "import nilearn.plotting as plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.plotting import plot_glass_brain\n",
    "\n",
    "from nilearn import glm\n",
    "import nilearn.image as nimg\n",
    "import nilearn.masking as masking\n",
    "from nilearn import plotting\n",
    "\n",
    "# from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-logic",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-prize",
   "metadata": {},
   "source": [
    "This notebook analyzes the data from the 22 participants in our study (23 collected, one excluded because of a head coil issue during scanning). 21 subjects' data was used in full and one subject had one run excluded, with 21 naive subjects and one with knowledge of the study. There were also a few subjects whose data was processed slightly differently due to differences in the scan sequence. For example, for two subjects, they initially saw a repeat experimental run in the run sequence, then completed the missing run afterwards (e.g. they might have seen runs 1, 2, 3, 4, 4, 6, then completed run 5 afterwards). These two subjects are handled in separate chunks of code at the bottom of this notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-czech",
   "metadata": {},
   "source": [
    "# Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caring-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET SUBJECT DIRECTORIES\n",
    "\n",
    "data_dir    = '../../data/bids/derivatives/fmriprep/'\n",
    "events_dir  = '../../data/behavioral/event_timing'\n",
    "results_dir = 'first_level_GLM'\n",
    "\n",
    "sub_dirs    = [x for x in os.listdir(data_dir) if 'sub' in x and '.html' not in x \n",
    "               and 'sub-112' not in x and 'sub-212' not in x\n",
    "               and 'sub-101' not in x and 'sub-201' not in x\n",
    "               and 'sub-017' not in x\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-parks",
   "metadata": {},
   "source": [
    "Confirm we have our 23 subjects minus 1 excluded and two unusual (total=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prime-browse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_dirs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-vegetarian",
   "metadata": {},
   "source": [
    "### load mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "present-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = nib.load('binary_shaef.nii')\n",
    "events_path = '../../data/behavioral/event_timing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-subscriber",
   "metadata": {},
   "source": [
    "# Create GLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blessed-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the first_level GLM model\n",
    "model = nl.glm.first_level.FirstLevelModel(\n",
    "                                          t_r=2,\n",
    "                                          signal_scaling=False,\n",
    "                                          hrf_model='glover',\n",
    "                                          drift_order=None,\n",
    "                                          mask_img=mask,\n",
    "                                          # drift_model='cosine',\n",
    "                                          drift_model=None,\n",
    "                                          # Isaac set to none\n",
    "                                          smoothing_fwhm=4,\n",
    "                                          standardize=True,\n",
    "                                          minimize_memory=False,\n",
    "                                          high_pass= .01 # default value is .01\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-assist",
   "metadata": {},
   "source": [
    "# _ buttons.tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interesting-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update timing labels for 1 & 2 timing files\n",
    "\n",
    "ej_files      = [ x for x in os.listdir(events_dir) if '_buttons.tsv' in x ] # 'events_judgement' in x ]\n",
    "ej_files_full = [ events_path + f for f in ej_files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-swing",
   "metadata": {},
   "source": [
    "### Prep the events files and contrasts for different GLM comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "biblical-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list   = ['labeled-type-videos']\n",
    "\n",
    "events_files  = [ x + '.tsv' for x in events_list ] \n",
    "\n",
    "contrast_list = ['veridical - scrambled']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-carroll",
   "metadata": {},
   "source": [
    "### load runs to exclude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brown-conspiracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>run</th>\n",
       "      <th>moviestim</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15</td>\n",
       "      <td>new MRI Behavioral Data/subject_21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  run  moviestim                             subject\n",
       "0           2  3.0         15  new MRI Behavioral Data/subject_21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../../data/behavioral/exclude_runs_behavioral.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-tractor",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "musical-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(image, mask=False, avg_mask=np.nan, full_output=False):\n",
    "    '''\n",
    "    inputs  : mask_type - string - mask_type (options: gm (grey matter), \n",
    "                                                    whole-brain, \n",
    "                                                    wm (white matter))\n",
    "              image     - string - path to mri data for a single run\n",
    "    outputs : masked_data - ???? - brain data with grey matter mask applied\n",
    "    '''\n",
    "    \n",
    "    # Load your fMRI data\n",
    "    fmri_image = nib.load(image)\n",
    "    \n",
    "    if mask == True:\n",
    "        \n",
    "        print(\"you're already passing mask into the model!\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        final_data = fmri_image.get_fdata()\n",
    "    \n",
    "    if full_output:\n",
    "        \n",
    "        affine = fmri_image.affine\n",
    "        header = fmri_image.header\n",
    "        shape  = final_data.shape\n",
    "        \n",
    "        return(fmri_image, final_data, affine, header, shape)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "regulated-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_runs(data_dir, subject, percent_frames=.30, fwd=.5, exclude=[]):\n",
    "    '''\n",
    "    input   :  data_dir  - string - path to subject directories\n",
    "               subject   - string - subject directory\n",
    "    output  :  good_runs - list   - list of runs with <30% of frames with motion (>=.5 FWD)\n",
    "    '''\n",
    "    \n",
    "    good_runs   = []\n",
    "    regressors  = []\n",
    "    \n",
    "    for r in ['01','02','03','04','05','06']:\n",
    "        \n",
    "        if r not in exclude:\n",
    "        \n",
    "            df      = pd.read_csv(data_dir+subject+'/ses-01/func/'+subject+'_ses-01_task-PredictingAttention_run-'+r+'_desc-confounds_timeseries.tsv', sep=\"\\t\")\n",
    "            percent = df[df['framewise_displacement']>=fwd].shape[0] / df.shape[0]\n",
    "\n",
    "            # if bad motion in less than 30% of frames --> good run\n",
    "            if percent < percent_frames:\n",
    "                good_runs.append(r)\n",
    "\n",
    "                l = list(df['framewise_displacement']>.5)\n",
    "                l_new = [int(x) for x in l]\n",
    "                regressors.append(l_new)\n",
    "        \n",
    "    return(good_runs,regressors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-powell",
   "metadata": {},
   "source": [
    "### Do the GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affiliated-requirement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we are excluding subject 21 run 3\n",
    "\n",
    "for subject in sub_dirs:  \n",
    "    \n",
    "    for events_file, contrast in zip( events_files, contrast_list ):\n",
    "        \n",
    "        new_confound_list = []\n",
    "            \n",
    "        print(subject); print(contrast)\n",
    "\n",
    "        if subject == 'sub-021':\n",
    "            # get runs without excessive motion, exclude run 3\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject, exclude=['03'])\n",
    "            print('excluded runs 3 for subject 21. included:')\n",
    "            print(runs)\n",
    "            \n",
    "        else:\n",
    "            # get runs without excessive motion\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject)\n",
    "        \n",
    "        # select the images for the good runs\n",
    "        image_list = [ data_dir+subject+'/ses-01/func/'+subject+'_ses-01_task-PredictingAttention_run-'+x+'_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' for x in runs ]\n",
    "\n",
    "        # get confounds\n",
    "        strategy   = [ 'motion','wm_csf','global_signal','compcor','scrub','high_pass' ]\n",
    "        confounds  = [ nl.interfaces.fmriprep.load_confounds(x, \n",
    "                                                            strategy = strategy, \n",
    "                                                            fd_threshold=0.5,\n",
    "                                                            n_compcor = 5,\n",
    "                                                            motion ='basic',\n",
    "                                                            demean = False) for x in image_list ]\n",
    "        confound_list = [x[0] for x in confounds]\n",
    "        \n",
    "        new_confound_list = confound_list \n",
    "        \n",
    "        # get sample masks\n",
    "        sm_list = [x[1] for x in confounds]\n",
    "\n",
    "        # replace any cases of None with array listing all frames, 0-109\n",
    "        sm_list = [np.arange(110) if x is None else x for x in sm_list]\n",
    "\n",
    "        # get data\n",
    "        full_data = []; affine = []; header = []; shape = []\n",
    "\n",
    "        for i in image_list:\n",
    "            the_image,f,a,h,s = get_data(i, mask=False, avg_mask=None, full_output=True) \n",
    "            affine.append(a); header.append(h); shape.append(s)            \n",
    "            full_data.append(the_image)\n",
    "\n",
    "        cleaned_images = full_data #[nib.Nifti1Image(a, affine=None) for a in full_data]\n",
    "\n",
    "        # load event timings\n",
    "        event_timing_files = [events_dir+'/'+subject+'_ses-01_task-PredictingAttention_run-'+r+'_'+events_file for r in runs]\n",
    "\n",
    "        event_dfs = [ pd.read_table(x) for x in event_timing_files ]\n",
    "\n",
    "        # make design matrices\n",
    "        design_matrices = []\n",
    "\n",
    "        for e,c in zip(event_dfs, confound_list):\n",
    "\n",
    "            TR_array   = np.array([x*2 for x in np.arange(110)])\n",
    "            matrix = glm.first_level.make_first_level_design_matrix(TR_array, \n",
    "                                                                    e, \n",
    "                                                                    hrf_model='glover', \n",
    "                                                                    drift_model=None,\n",
    "                                                                    add_regs = c)\n",
    "            design_matrices.append(matrix)\n",
    "            print('ready to fit the model')\n",
    "\n",
    "\n",
    "        print(cleaned_images[0].shape)\n",
    "        print(new_confound_list[0].shape)\n",
    "        print(design_matrices[0].shape)\n",
    "        print(event_dfs[0].shape)\n",
    "        print(sm_list[0].shape)\n",
    "\n",
    "        # fit the model\n",
    "        fitted_model = model.fit(cleaned_images, \n",
    "                                 confounds       = new_confound_list, \n",
    "                                 design_matrices = design_matrices,\n",
    "                                 events          = event_dfs,\n",
    "                                 sample_masks    = sm_list) \n",
    "\n",
    "        print('ready to compute contrasts')\n",
    "        # plot the result\n",
    "        z_score  = fitted_model.compute_contrast(contrast, output_type='z_score')\n",
    "        eff_size = fitted_model.compute_contrast(contrast, output_type='effect_size')\n",
    "        # NOTE: we'll use effect_size for second-level GLM\n",
    "        print('contrasts computed')\n",
    "\n",
    "        # make folder path\n",
    "        path_base = 'first_level_GLM' \n",
    "        directory_path = path_base \n",
    "\n",
    "        print('directory path created')\n",
    "\n",
    "        # make folder for this batch, if it doesn't exist\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.makedirs(directory_path)\n",
    "            print(\"Directory created:\", directory_path)\n",
    "        else:\n",
    "            print(\"Directory already exists:\", directory_path)\n",
    "\n",
    "        # contrast for filename\n",
    "        contrast_label = contrast.replace(' ','-')\n",
    "\n",
    "        # timing label\n",
    "        now   = datetime.datetime.now()\n",
    "        ctime = now.ctime()\n",
    "        date_time = ctime.replace(' ','-')\n",
    "\n",
    "        # save contrasts\n",
    "        z_score.to_filename(directory_path + '/' + events_file[:-4] + '_' + contrast_label + '_' + date_time + '_' + str(subject) + '_z-score.nii.gz')\n",
    "        eff_size.to_filename(directory_path + '/' + events_file[:-4] + '_' + contrast_label + '_' + date_time + '_' + str(subject) + '_eff-size.nii.gz')\n",
    "\n",
    "        print('contrasts saved out')\n",
    "        print(directory_path + '/' + events_file[:-4] + '_' + contrast_label + '_' + date_time + '_' + str(subject) + '_z-score.nii.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-application",
   "metadata": {},
   "source": [
    "### Now unusual subjects !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-gender",
   "metadata": {},
   "source": [
    "#### Subject 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "utility-mailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject</td>\n",
       "      <td>subject_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>behavioral_runs</td>\n",
       "      <td>['1', '2', '3', '4', '4', '5', '6']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                    0\n",
       "0          subject                            subject_1\n",
       "1  behavioral_runs  ['1', '2', '3', '4', '4', '5', '6']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1 = pd.read_csv('../../data/behavioral/subject_1_unusual_run_order.csv')\n",
    "sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fourth-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject 101 has runs labeled 01, 02, 03, 04, 05, 06\n",
    "\n",
    "# they correspond to           1,  2,  3,  4, drop, 5\n",
    "\n",
    "\n",
    "\n",
    "# subject 201 has runs labeled 01\n",
    "\n",
    "# they correspond to           6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handled-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start loading the data for subject 101\n",
    "\n",
    "# we can treat everything normally for runs 1-4, so let's exclude the rest for now\n",
    "\n",
    "for subject in ['sub-101']:  \n",
    "    \n",
    "    for events_file, contrast in zip( events_files, contrast_list ):\n",
    "        \n",
    "        new_confound_list = []\n",
    "        \n",
    "        path_base = 'first_level_GLM' \n",
    "            \n",
    "        print(subject)\n",
    "\n",
    "        if subject == 'sub-101':\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject, exclude=['05','06'])\n",
    "            print('excluded runs 5, 6 for subject 101. included:')\n",
    "            print(runs)\n",
    "            \n",
    "        \n",
    "        # select the images for the good runs\n",
    "        image_list = [ data_dir+subject+'/ses-01/func/'+subject+'_ses-01_task-PredictingAttention_run-'+x+'_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' for x in ['01','02','03','04'] ]\n",
    "        \n",
    "        # get confounds\n",
    "        strategy   = [ 'motion','wm_csf','global_signal','compcor','scrub','high_pass' ]\n",
    "        confounds  = [ nl.interfaces.fmriprep.load_confounds(x, \n",
    "                                                            strategy = strategy, \n",
    "                                                            fd_threshold=0.5,\n",
    "                                                            n_compcor = 5,\n",
    "                                                            motion ='basic',\n",
    "                                                            demean = False) for x in image_list ]\n",
    "        confound_list = [x[0] for x in confounds]\n",
    "        \n",
    "        new_confound_list = confound_list \n",
    "        \n",
    "        # commented lines below because we already do headscrubbing with nilearn\n",
    "        \n",
    "#         for add,c in zip(add_regressor_columns,confound_list):\n",
    "#             d = pd.DataFrame({'fwd':add})\n",
    "#             new_confound_list.append(pd.concat([c,d],axis=1))\n",
    "\n",
    "        # get sample masks\n",
    "        sm_list = [x[1] for x in confounds]\n",
    "\n",
    "        # replace any cases of None with array listing all frames, 0-109\n",
    "        sm_list = [np.arange(110) if x is None else x for x in sm_list]\n",
    "\n",
    "        # get data\n",
    "        full_data = []; affine = []; header = []; shape = []\n",
    "\n",
    "        for i in image_list:\n",
    "            the_image,f,a,h,s = get_data(i, mask=False, avg_mask=None, full_output=True) \n",
    "            affine.append(a); header.append(h); shape.append(s)            \n",
    "            full_data.append(the_image)\n",
    "\n",
    "        cleaned_images = full_data #[nib.Nifti1Image(a, affine=None) for a in full_data]\n",
    "\n",
    "        # load event timings\n",
    "        event_timing_files = [events_dir+'/sub-001_ses-01_task-PredictingAttention_run-'+r+'_'+events_file for r in runs]\n",
    "\n",
    "        event_dfs = [ pd.read_table(x) for x in event_timing_files ]\n",
    "\n",
    "        # make design matrices\n",
    "        design_matrices = []\n",
    "\n",
    "        for e,c in zip(event_dfs, confound_list):\n",
    "\n",
    "            TR_array   = np.array([x*2 for x in np.arange(110)])\n",
    "            matrix = glm.first_level.make_first_level_design_matrix(TR_array, \n",
    "                                                                    e, \n",
    "                                                                    hrf_model='glover', \n",
    "                                                                    drift_model=None,\n",
    "                                                                    add_regs = c)\n",
    "            design_matrices.append(matrix)\n",
    "            print('ready to fit the model')\n",
    "\n",
    "\n",
    "        print(cleaned_images[0].shape)\n",
    "        print(new_confound_list[0].shape)\n",
    "        print(design_matrices[0].shape)\n",
    "        print(event_dfs[0].shape)\n",
    "        print(sm_list[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "latin-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "o_cleaned_images    = cleaned_images\n",
    "o_new_confound_list = new_confound_list\n",
    "o_design_matrices   = design_matrices\n",
    "o_event_dfs         = event_dfs\n",
    "o_sm_list           = sm_list\n",
    "\n",
    "print(len(cleaned_images))\n",
    "print(len(new_confound_list))\n",
    "print(len(design_matrices))\n",
    "print(len(event_dfs))\n",
    "print(len(sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "utility-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded runs 1, 2, 3, 4, 5 for subject 101. included:\n",
      "['06']\n",
      "1\n",
      "ready to fit the model\n",
      "(78, 93, 78, 110)\n",
      "(110, 16)\n",
      "(110, 19)\n",
      "(20, 5)\n",
      "(110,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n"
     ]
    }
   ],
   "source": [
    "# now we'll add the fifth element:\n",
    "# it will be run 6 of the MRI data\n",
    "# it will be run 5 of the timing data\n",
    "\n",
    "for subject in ['sub-101']:  \n",
    "\n",
    "    \n",
    "    for events_file, contrast in zip( events_files, contrast_list ):\n",
    "        \n",
    "        new_confound_list = []\n",
    "\n",
    "        if subject == 'sub-101':\n",
    "            # get runs without excessive motion, exclude runs 2, 3, 4\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject, exclude=['01','02','03','04','05'])\n",
    "            print('excluded runs 1, 2, 3, 4, 5 for subject 101. included:')\n",
    "            print(runs)\n",
    "        \n",
    "        # select the images for the good runs\n",
    "        image_list = [ data_dir+subject+'/ses-01/func/sub-101_ses-01_task-PredictingAttention_run-06_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' ]\n",
    "        print(len(image_list))\n",
    "        \n",
    "        # get confounds\n",
    "        strategy   = [ 'motion','wm_csf','global_signal','compcor','scrub','high_pass' ]\n",
    "        confounds  = [ nl.interfaces.fmriprep.load_confounds(x, \n",
    "                                                            strategy = strategy, \n",
    "                                                            fd_threshold=0.5,\n",
    "                                                            n_compcor = 5,\n",
    "                                                            motion ='basic',\n",
    "                                                            demean = False) for x in image_list ]\n",
    "\n",
    "        confound_list = [x[0] for x in confounds]\n",
    "        \n",
    "        new_confound_list = confound_list \n",
    "        \n",
    "        # commented lines below because we already do headscrubbing with nilearn\n",
    "        \n",
    "#         for add,c in zip(add_regressor_columns,confound_list):\n",
    "#             d = pd.DataFrame({'fwd':add})\n",
    "#             new_confound_list.append(pd.concat([c,d],axis=1))\n",
    "\n",
    "        # get sample masks\n",
    "        sm_list = [x[1] for x in confounds]\n",
    "\n",
    "        # replace any cases of None with array listing all frames, 0-109\n",
    "        sm_list = [np.arange(110) if x is None else x for x in sm_list]\n",
    "\n",
    "        # get data\n",
    "        full_data = []; affine = []; header = []; shape = []\n",
    "\n",
    "        for i in image_list:\n",
    "            the_image,f,a,h,s = get_data(i, mask=False, avg_mask=None, full_output=True) \n",
    "            affine.append(a); header.append(h); shape.append(s)            \n",
    "            full_data.append(the_image)\n",
    "\n",
    "        cleaned_images = full_data #[nib.Nifti1Image(a, affine=None) for a in full_data]\n",
    "\n",
    "        # load event timings\n",
    "        event_timing_files = [events_dir+'/sub-001_ses-01_task-PredictingAttention_run-05_'+events_file for r in runs]\n",
    "\n",
    "        event_dfs = [ pd.read_table(x) for x in event_timing_files ]\n",
    "\n",
    "        # make design matrices\n",
    "        design_matrices = []\n",
    "\n",
    "        for e,c in zip(event_dfs, confound_list):\n",
    "\n",
    "            TR_array   = np.array([x*2 for x in np.arange(110)])\n",
    "            matrix = glm.first_level.make_first_level_design_matrix(TR_array, \n",
    "                                                                    e, \n",
    "                                                                    hrf_model='glover', \n",
    "                                                                    drift_model=None,\n",
    "                                                                    add_regs = c)\n",
    "            design_matrices.append(matrix)\n",
    "            print('ready to fit the model')\n",
    "\n",
    "\n",
    "        print(cleaned_images[0].shape)\n",
    "        print(new_confound_list[0].shape)\n",
    "        print(design_matrices[0].shape)\n",
    "        print(event_dfs[0].shape)\n",
    "        print(sm_list[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mighty-surgery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_images))\n",
    "print(len(new_confound_list))\n",
    "print(len(design_matrices))\n",
    "print(len(event_dfs))\n",
    "print(len(sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coordinate-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(o_cleaned_images))\n",
    "print(len(o_new_confound_list))\n",
    "print(len(o_design_matrices))\n",
    "print(len(o_event_dfs))\n",
    "print(len(o_sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "exciting-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_cleaned_images.append(cleaned_images[0])\n",
    "o_new_confound_list.append(new_confound_list[0])\n",
    "o_design_matrices.append(design_matrices[0])\n",
    "o_event_dfs.append(event_dfs[0])\n",
    "o_sm_list.append(sm_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "noted-appeal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(o_cleaned_images))\n",
    "print(len(o_new_confound_list))\n",
    "print(len(o_design_matrices))\n",
    "print(len(o_event_dfs))\n",
    "print(len(o_sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "chubby-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded runs 2, 3, 4, 5, 6 for subject 12. included:\n",
      "['01']\n",
      "ready to fit the model\n",
      "(78, 93, 78, 110)\n",
      "(110, 16)\n",
      "(110, 19)\n",
      "(20, 5)\n",
      "(108,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n"
     ]
    }
   ],
   "source": [
    "# now we'll add the sixth element:\n",
    "# it will be run 1 of the MRI data for subject 201\n",
    "# it will be run 6 of the timing data for subject 001\n",
    "\n",
    "for subject in ['sub-201']:  \n",
    "    \n",
    "    for events_file, contrast in zip( events_files, contrast_list ):\n",
    "        \n",
    "        new_confound_list = [] \n",
    "        \n",
    "        if subject == 'sub-201':\n",
    "            # get runs without excessive motion, exclude runs 2, 3, 4\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject, exclude=['02','03','04','05','06'])\n",
    "            print('excluded runs 2, 3, 4, 5, 6. included:')\n",
    "            print(runs)\n",
    "            \n",
    "        else:\n",
    "            # get runs without excessive motion\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject)\n",
    "        \n",
    "        # select the images for the good runs\n",
    "        image_list = [ data_dir+subject+'/ses-01/func/sub-201_ses-01_task-PredictingAttention_run-01_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' ]\n",
    "        \n",
    "        # get confounds\n",
    "        strategy   = [ 'motion','wm_csf','global_signal','compcor','scrub','high_pass' ]\n",
    "        confounds  = [ nl.interfaces.fmriprep.load_confounds(x, \n",
    "                                                            strategy = strategy, \n",
    "                                                            fd_threshold=0.5,\n",
    "                                                            n_compcor = 5,\n",
    "                                                            motion ='basic',\n",
    "                                                            demean = False) for x in image_list ]\n",
    "        confound_list = [x[0] for x in confounds]\n",
    "        \n",
    "        new_confound_list = confound_list \n",
    "        \n",
    "        # commented lines below because we already do headscrubbing with nilearn\n",
    "        \n",
    "#         for add,c in zip(add_regressor_columns,confound_list):\n",
    "#             d = pd.DataFrame({'fwd':add})\n",
    "#             new_confound_list.append(pd.concat([c,d],axis=1))\n",
    "\n",
    "        # get sample masks\n",
    "        sm_list = [x[1] for x in confounds]\n",
    "\n",
    "        # replace any cases of None with array listing all frames, 0-109\n",
    "        sm_list = [np.arange(110) if x is None else x for x in sm_list]\n",
    "\n",
    "        # get data\n",
    "        full_data = []; affine = []; header = []; shape = []\n",
    "\n",
    "        for i in image_list:\n",
    "            the_image,f,a,h,s = get_data(i, mask=False, avg_mask=None, full_output=True) \n",
    "            affine.append(a); header.append(h); shape.append(s)            \n",
    "            full_data.append(the_image)\n",
    "\n",
    "        cleaned_images = full_data #[nib.Nifti1Image(a, affine=None) for a in full_data]\n",
    "\n",
    "        # load event timings\n",
    "        event_timing_files = [events_dir+'/sub-001_ses-01_task-PredictingAttention_run-06_'+events_file for r in runs]\n",
    "\n",
    "        event_dfs = [ pd.read_table(x) for x in event_timing_files ]\n",
    "\n",
    "        # make design matrices\n",
    "        design_matrices = []\n",
    "\n",
    "        for e,c in zip(event_dfs, confound_list):\n",
    "\n",
    "            TR_array   = np.array([x*2 for x in np.arange(110)])\n",
    "            matrix = glm.first_level.make_first_level_design_matrix(TR_array, \n",
    "                                                                    e, \n",
    "                                                                    hrf_model='glover', \n",
    "                                                                    drift_model=None,\n",
    "                                                                    add_regs = c)\n",
    "            design_matrices.append(matrix)\n",
    "            print('ready to fit the model')\n",
    "\n",
    "\n",
    "        print(cleaned_images[0].shape)\n",
    "        print(new_confound_list[0].shape)\n",
    "        print(design_matrices[0].shape)\n",
    "        print(event_dfs[0].shape)\n",
    "        print(sm_list[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "neural-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_images))\n",
    "print(len(new_confound_list))\n",
    "print(len(design_matrices))\n",
    "print(len(event_dfs))\n",
    "print(len(sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "international-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(o_cleaned_images))\n",
    "print(len(o_new_confound_list))\n",
    "print(len(o_design_matrices))\n",
    "print(len(o_event_dfs))\n",
    "print(len(o_sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "packed-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_cleaned_images.append(cleaned_images[0])\n",
    "o_new_confound_list.append(new_confound_list[0])\n",
    "o_design_matrices.append(design_matrices[0])\n",
    "o_event_dfs.append(event_dfs[0])\n",
    "o_sm_list.append(sm_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frozen-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "fitted_model = model.fit(o_cleaned_images, \n",
    "                         confounds       = o_new_confound_list, \n",
    "                         design_matrices = o_design_matrices,\n",
    "                         events          = o_event_dfs,\n",
    "                         sample_masks    = o_sm_list) \n",
    "\n",
    "print('ready to compute contrasts')\n",
    "# plot the result\n",
    "z_score  = fitted_model.compute_contrast(contrast, output_type='z_score')\n",
    "eff_size = fitted_model.compute_contrast(contrast, output_type='effect_size')\n",
    "# NOTE: we'll use z_score for visualizing and effect_size for second-level GLM\n",
    "print('contrasts computed')\n",
    "\n",
    "# make folder path\n",
    "path_base = 'first_level_GLM' \n",
    "directory_path = path_base \n",
    "\n",
    "print('directory path created')\n",
    "\n",
    "# make folder for this batch, if it doesn't exist\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(\"Directory created:\", directory_path)\n",
    "else:\n",
    "    print(\"Directory already exists:\", directory_path)\n",
    "\n",
    "# contrast for filename\n",
    "contrast_label = contrast.replace(' ','-')\n",
    "\n",
    "# timing label\n",
    "now   = datetime.datetime.now()\n",
    "ctime = now.ctime()\n",
    "date_time = ctime.replace(' ','-')\n",
    "\n",
    "# save contrasts\n",
    "z_score.to_filename(directory_path + '/' + events_file[:-4] + '_' + contrast_label + '_' + date_time + '_' + 'sub-001' + '_z-score.nii.gz')\n",
    "eff_size.to_filename(directory_path + '/' + events_file[:-4] + '_' + contrast_label + '_' + date_time + '_' + 'sub-001' + '_eff-size.nii.gz')\n",
    "\n",
    "print('contrasts saved out')\n",
    "print(directory_path + '/' + contrast_label + '_' + date_time + '_' + 'sub-001' + '_z-score.nii.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-masters",
   "metadata": {},
   "source": [
    "### Now, subject 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "divided-civilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject</td>\n",
       "      <td>subject_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>behavioral_runs</td>\n",
       "      <td>['1', '2', '3', '3', '5', '6', '4']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                    1\n",
       "0          subject                           subject_12\n",
       "1  behavioral_runs  ['1', '2', '3', '3', '5', '6', '4']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1 = pd.read_csv('../../data/behavioral/subject_12_unusual_run_order.csv')\n",
    "sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "indirect-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject 212 has runs labeled 01, 02, 03, 04, 05, 06\n",
    "\n",
    "# they correspond to           1,  2,  3,  drop, 5, 6\n",
    "\n",
    "\n",
    "\n",
    "# subject 201 has runs labeled 01\n",
    "\n",
    "# they correspond to           4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "supported-footwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-212\n",
      "veridical---scrambled\n",
      "excluded run 4 for subject 12. included:\n",
      "['01', '02', '03', '05', '06']\n",
      "ready to fit the model\n",
      "ready to fit the model\n",
      "ready to fit the model\n",
      "ready to fit the model\n",
      "ready to fit the model\n",
      "(78, 93, 78, 110)\n",
      "(110, 16)\n",
      "(110, 19)\n",
      "(20, 5)\n",
      "(110,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n",
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n",
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n",
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n",
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n"
     ]
    }
   ],
   "source": [
    "# let's start loading the data for subject 212\n",
    "\n",
    "for subject in ['sub-212']:  \n",
    "    \n",
    "    for events_file, contrast in zip( events_files, contrast_list ):\n",
    "        \n",
    "        new_confound_list = []\n",
    "        \n",
    "        \n",
    "        ######## THIS CODE IS TO GET NEW SUBJECTS WHO WERE MISSING BEFORE ########\n",
    "        \n",
    "        # only uncomment if incorporating new subjects into existing first-level analysis\n",
    "        \n",
    "        # make folder path\n",
    "        path_base = 'first_level_GLM' \n",
    "        directory_path = path_base \n",
    "        \n",
    "        contrast_label = contrast.replace(' ','-')\n",
    "        #check_file     = [ x for x in os.listdir(directory_path) if str(subject) in x and contrast_label in x and '_eff-size.nii.gz' in x ]\n",
    "        \n",
    "        ###########################################################################\n",
    "            \n",
    "        print(subject); print(contrast_label)\n",
    "\n",
    "        if subject == 'sub-212':\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject, exclude=['04'])\n",
    "            print('excluded run 4 for subject 12. included:')\n",
    "            print(runs)\n",
    "            \n",
    "        else:\n",
    "            # get runs without excessive motion\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject)\n",
    "        \n",
    "        # select the images for the good runs\n",
    "        image_list = [ data_dir+subject+'/ses-01/func/'+subject+'_ses-01_task-PredictingAttention_run-'+x+'_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' for x in ['01','02','03','05','06'] ]\n",
    "        \n",
    "        # get confounds\n",
    "        strategy   = [ 'motion','wm_csf','global_signal','compcor','scrub','high_pass' ]\n",
    "        confounds  = [ nl.interfaces.fmriprep.load_confounds(x, \n",
    "                                                            strategy = strategy, \n",
    "                                                            fd_threshold=0.5,\n",
    "                                                            n_compcor = 5,\n",
    "                                                            motion ='basic',\n",
    "                                                            demean = False) for x in image_list ]\n",
    "        confound_list = [x[0] for x in confounds]\n",
    "        \n",
    "        new_confound_list = confound_list \n",
    "        \n",
    "        # commented lines below because we already do headscrubbing with nilearn\n",
    "        \n",
    "#         for add,c in zip(add_regressor_columns,confound_list):\n",
    "#             d = pd.DataFrame({'fwd':add})\n",
    "#             new_confound_list.append(pd.concat([c,d],axis=1))\n",
    "\n",
    "        # get sample masks\n",
    "        sm_list = [x[1] for x in confounds]\n",
    "\n",
    "        # replace any cases of None with array listing all frames, 0-109\n",
    "        sm_list = [np.arange(110) if x is None else x for x in sm_list]\n",
    "\n",
    "        # get data\n",
    "        full_data = []; affine = []; header = []; shape = []\n",
    "\n",
    "        for i in image_list:\n",
    "            the_image,f,a,h,s = get_data(i, mask=False, avg_mask=None, full_output=True) \n",
    "            affine.append(a); header.append(h); shape.append(s)            \n",
    "            full_data.append(the_image)\n",
    "\n",
    "        cleaned_images = full_data #[nib.Nifti1Image(a, affine=None) for a in full_data]\n",
    "\n",
    "        # load event timings\n",
    "        event_timing_files = [events_dir+'/sub-012_ses-01_task-PredictingAttention_run-'+r+'_'+events_file for r in runs]\n",
    "\n",
    "        event_dfs = [ pd.read_table(x) for x in event_timing_files ]\n",
    "\n",
    "        # make design matrices\n",
    "        design_matrices = []\n",
    "\n",
    "        for e,c in zip(event_dfs, confound_list):\n",
    "\n",
    "            TR_array   = np.array([x*2 for x in np.arange(110)])\n",
    "            matrix = glm.first_level.make_first_level_design_matrix(TR_array, \n",
    "                                                                    e, \n",
    "                                                                    hrf_model='glover', \n",
    "                                                                    drift_model=None,\n",
    "                                                                    add_regs = c)\n",
    "            design_matrices.append(matrix)\n",
    "            print('ready to fit the model')\n",
    "\n",
    "\n",
    "        print(cleaned_images[0].shape)\n",
    "        print(new_confound_list[0].shape)\n",
    "        print(design_matrices[0].shape)\n",
    "        print(event_dfs[0].shape)\n",
    "        print(sm_list[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "disciplinary-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "o_cleaned_images    = cleaned_images\n",
    "o_new_confound_list = new_confound_list\n",
    "o_design_matrices   = design_matrices\n",
    "o_event_dfs         = event_dfs\n",
    "o_sm_list           = sm_list\n",
    "\n",
    "print(len(cleaned_images))\n",
    "print(len(new_confound_list))\n",
    "print(len(design_matrices))\n",
    "print(len(event_dfs))\n",
    "print(len(sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "forced-extreme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluded runs 2, 3, 4, 5, 6 for subject 12. included:\n",
      "['01']\n",
      "ready to fit the model\n",
      "(78, 93, 78, 110)\n",
      "(110, 16)\n",
      "(110, 19)\n",
      "(20, 5)\n",
      "(108,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/people/kz0108/.local/lib/python3.7/site-packages/nilearn/glm/first_level/experimental_paradigm.py:167: UserWarning: The following unexpected columns in events data will be ignored: response_time, stim_file\n",
      "  \"The following unexpected columns \"\n"
     ]
    }
   ],
   "source": [
    "# now we'll add the sixth element:\n",
    "# it will be run 1 of the MRI data for subject 112\n",
    "# it will be run 4 of the timing data for subject 12\n",
    "\n",
    "for subject in ['sub-112']:  \n",
    "    \n",
    "    for events_file, contrast in zip( events_files, contrast_list ):\n",
    "        \n",
    "        new_confound_list = [] \n",
    "        \n",
    "        if subject == 'sub-112':\n",
    "            # get runs without excessive motion, exclude runs 2, 3, 4\n",
    "            runs,add_regressor_columns = get_good_runs(data_dir, subject, exclude=['02','03','04','05','06'])\n",
    "            print('excluded runs 2, 3, 4, 5, 6 for subject 12. included:')\n",
    "            print(runs)\n",
    "        \n",
    "        # select the images for the good runs\n",
    "        image_list = [ data_dir+subject+'/ses-01/func/sub-112_ses-01_task-PredictingAttention_run-01_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz' ]\n",
    "        \n",
    "        # get confounds\n",
    "        strategy   = [ 'motion','wm_csf','global_signal','compcor','scrub','high_pass' ]\n",
    "        confounds  = [ nl.interfaces.fmriprep.load_confounds(x, \n",
    "                                                            strategy = strategy, \n",
    "                                                            fd_threshold=0.5,\n",
    "                                                            n_compcor = 5,\n",
    "                                                            motion ='basic',\n",
    "                                                            demean = False) for x in image_list ]\n",
    "        confound_list = [x[0] for x in confounds]\n",
    "        \n",
    "        new_confound_list = confound_list \n",
    "        \n",
    "        # commented lines below because we already do headscrubbing with nilearn\n",
    "        \n",
    "#         for add,c in zip(add_regressor_columns,confound_list):\n",
    "#             d = pd.DataFrame({'fwd':add})\n",
    "#             new_confound_list.append(pd.concat([c,d],axis=1))\n",
    "\n",
    "        # get sample masks\n",
    "        sm_list = [x[1] for x in confounds]\n",
    "\n",
    "        # replace any cases of None with array listing all frames, 0-109\n",
    "        sm_list = [np.arange(110) if x is None else x for x in sm_list]\n",
    "\n",
    "        # get data\n",
    "        full_data = []; affine = []; header = []; shape = []\n",
    "\n",
    "        for i in image_list:\n",
    "            the_image,f,a,h,s = get_data(i, mask=False, avg_mask=None, full_output=True) \n",
    "            affine.append(a); header.append(h); shape.append(s)            \n",
    "            full_data.append(the_image)\n",
    "\n",
    "        cleaned_images = full_data #[nib.Nifti1Image(a, affine=None) for a in full_data]\n",
    "\n",
    "        # load event timings\n",
    "        event_timing_files = [events_dir+'/sub-012_ses-01_task-PredictingAttention_run-04_'+events_file for r in runs]\n",
    "\n",
    "        event_dfs = [ pd.read_table(x) for x in event_timing_files ]\n",
    "\n",
    "        # make design matrices\n",
    "        design_matrices = []\n",
    "\n",
    "        for e,c in zip(event_dfs, confound_list):\n",
    "\n",
    "            TR_array   = np.array([x*2 for x in np.arange(110)])\n",
    "            matrix = glm.first_level.make_first_level_design_matrix(TR_array, \n",
    "                                                                    e, \n",
    "                                                                    hrf_model='glover', \n",
    "                                                                    drift_model=None,\n",
    "                                                                    add_regs = c)\n",
    "            design_matrices.append(matrix)\n",
    "            print('ready to fit the model')\n",
    "\n",
    "\n",
    "        print(cleaned_images[0].shape)\n",
    "        print(new_confound_list[0].shape)\n",
    "        print(design_matrices[0].shape)\n",
    "        print(event_dfs[0].shape)\n",
    "        print(sm_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "musical-phase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_images))\n",
    "print(len(new_confound_list))\n",
    "print(len(design_matrices))\n",
    "print(len(event_dfs))\n",
    "print(len(sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fifty-division",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(o_cleaned_images))\n",
    "print(len(o_new_confound_list))\n",
    "print(len(o_design_matrices))\n",
    "print(len(o_event_dfs))\n",
    "print(len(o_sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "smooth-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_cleaned_images.append(cleaned_images[0])\n",
    "o_new_confound_list.append(new_confound_list[0])\n",
    "o_design_matrices.append(design_matrices[0])\n",
    "o_event_dfs.append(event_dfs[0])\n",
    "o_sm_list.append(sm_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "numerous-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(o_cleaned_images))\n",
    "print(len(o_new_confound_list))\n",
    "print(len(o_design_matrices))\n",
    "print(len(o_event_dfs))\n",
    "print(len(o_sm_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subtle-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "fitted_model = model.fit(o_cleaned_images, \n",
    "                         confounds       = o_new_confound_list, \n",
    "                         design_matrices = o_design_matrices,\n",
    "                         events          = o_event_dfs,\n",
    "                         sample_masks    = o_sm_list) \n",
    "\n",
    "print('ready to compute contrasts')\n",
    "# plot the result\n",
    "z_score  = fitted_model.compute_contrast(contrast, output_type='z_score')\n",
    "eff_size = fitted_model.compute_contrast(contrast, output_type='effect_size')\n",
    "# NOTE: we'll use z_score for visualizing and effect_size for second-level GLM\n",
    "print('contrasts computed')\n",
    "\n",
    "# make folder path\n",
    "path_base = 'first_level_GLM' \n",
    "directory_path = path_base \n",
    "\n",
    "print('directory path created')\n",
    "\n",
    "# make folder for this batch, if it doesn't exist\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(\"Directory created:\", directory_path)\n",
    "else:\n",
    "    print(\"Directory already exists:\", directory_path)\n",
    "\n",
    "# contrast for filename\n",
    "contrast_label = contrast.replace(' ','-')\n",
    "\n",
    "# timing label\n",
    "now   = datetime.datetime.now()\n",
    "ctime = now.ctime()\n",
    "date_time = ctime.replace(' ','-')\n",
    "\n",
    "# save contrasts\n",
    "z_score.to_filename(directory_path + '/' + events_file[:-4] + '_' + contrast_label + '_' + date_time + '_' + 'sub-012' + '_z-score.nii.gz')\n",
    "eff_size.to_filename(directory_path + '/' + events_file[:-4] + '_' + contrast_label + '_' + date_time + '_' + 'sub-012' + '_eff-size.nii.gz')\n",
    "\n",
    "print('contrasts saved out')\n",
    "print(directory_path + '/' + contrast_label + '_' + date_time + '_' + 'sub-012' + '_z-score.nii.gz')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
